
- hosts:
    - mon
    - osd1
    - osd2
  become: true

  vars:
    external_device: "{{ logicalvolume['external_device'] }}"
    partition_size: "{{ logicalvolume['partition_size'] }}GB"
    vg: "{{ logicalvolume['vg_name'] }}"
    partition_index: "{{ logicalvolume['external_device'] }}{{ logicalvolume['partition_index'] }}"
    lv: "{{ logicalvolume['lv_name'] }}"
    lvm_size: "{{ logicalvolume['lvm_size'] }}g"

  pre_tasks:
    - name: exit playbook, if the admin devices not exit
      fail:
        mag: "You must Define at least one admin node"
      when: admin not in groups

    - name: exit playbook, if the admin monitor and osd devices not exit
      fail:
        mag: "You must Define at least one monitor node"
      when: mons not in groups

    - name: exit playbook, if the admin monitor and osd devices not exit
      fail:
        mag: "You must Define at least one osd node"
      when: osds not in groups

    - import_task: create_lvm.yml
  
  tasks:
    - name: set_facts for the logical volumes
      set_facts:
        external_device: "{{ logicalvolume['external_device'] }}"
        partition_size: "{{ logicalvolume['partition_size'] }}GB"
        vg: "{{ logicalvolume['vg_name'] }}"
        partition_index: "{{ logicalvolume['external_device'] }}{{ logicalvolume['partition_index'] }}"
        lv: "{{ logicalvolume['lv_name'] }}"
        lvm_size: "{{ logicalvolume['lvm_size'] }}g"

    - import_task: create_lvm.yml

- hosts:
    - admin
    - mons
    - osd1
    - osd2
  pre_tasks:
    - import_role:
        name: ceph-prerun
      when: ansible_distribution == "Debian"
  tasks:
    - import_role:
        name: ceph-setup




# pvcreate /dev/vdd && vgcreate vg_ceph /dev/vdd && lvcreate -L9216 -n lv_ceph vg_ceph && mkfs.ext4 /dev/vg_ceph/lv_ceph

# lvs

# hostnamectl set-hostname ceph-admin
# hostnamectl set-hostname ceph-mon
# hostnamectl set-hostname ceph-osd1
# hostnamectl set-hostname ceph-osd2

# vi /etc/hosts

# 10.204.68.46 ceph-admin
# 10.204.68.47 ceph-mon
# 10.204.68.48 ceph-osd1
# 10.204.68.49 ceph-osd2

# apt install chrony -y

# vim /etc/chrony/chrony.conf


# pool dk.pool.ntp.org iburst

# systemctl restart chronyd

# apt install openssh-server

# systemctl enable --now ssh.socket

# useradd -m -s /bin/bash cephadmin

# passwd cephadmin

# ceph123!

# echo "cephadmin ALL=(ALL:ALL) NOPASSWD:ALL" >> /etc/sudoers.d/cephadmin

# chmod 0440 /etc/sudoers.d/cephadmin

# #################################
# ######### From all node #########
# #################################

# sudo apt install apt-transport-https ca-certificates curl gnupg-agent software-properties-common -y && curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - && echo "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -sc) stable" | sudo tee /etc/apt/sources.list.d/docker-ce.list && sudo apt update && sudo apt install docker-ce docker-ce-cli containerd.io -y && sudo systemctl enable --now docker


# #####################################
# ######### all, except admin #########
# #####################################

# vim /etc/ssh/sshd_config

# Match Address 10.204.68.46
#         PermitRootLogin yes

# systemctl reload ssh.socket


# #########################################
# ######### install ceph on admin #########
# #########################################

# sudo wget -q https://github.com/ceph/ceph/raw/pacific/src/cephadm/cephadm -P /usr/bin/

# sudo chmod +x /usr/bin/cephadm

# su - cephadmin
# whoami
# sudo cephadm bootstrap --mon-ip 10.204.68.46

# sudo docker ps

# sudo /usr/bin/cephadm shell --fsid f959b65e-91c2-11ec-9776-abbffb8a52a1 -c /etc/ceph/ceph.conf -k /etc/ceph/ceph.client.admin.keyring

# 27c398a8-c15b-11ec-8aed-7b4e98e4ac70

# sudo /usr/bin/cephadm shell --fsid 27c398a8-c15b-11ec-8aed-7b4e98e4ac70 -c /etc/ceph/ceph.conf -k /etc/ceph/ceph.client.admin.keyring

# sudo ceph -s

# sudo ceph orch host ls

# sudo ssh-copy-id -f -i /etc/ceph/ceph.pub root@ceph-mon

# sudo ceph orch host add ceph-mon

# for i in ceph-osd1 ceph-osd2; do sudo ssh-copy-id -f -i /etc/ceph/ceph.pub root@$i; done

# sudo ceph orch host add ceph-osd1
# sudo ceph orch host add ceph-osd2
# for i in ceph-osd1 ceph-osd2; do sudo ceph orch host label add $i osd; done


# #################################################
# ######### create logical volume for osd #########
# #################################################

# pvcreate /dev/vde && vgcreate vg_ceph_20 /dev/vde && lvcreate -L19456 -n lv_ceph_20 vg_ceph_20 && mkfs.ext4 /dev/vg_ceph_20/lv_ceph_20


# sudo ceph orch daemon add osd ceph-mon:vg_ceph_20/lv_ceph_20

# sudo ceph orch daemon add osd ceph-osd1:vg_ceph_20/lv_ceph_20

# sudo ceph orch daemon add osd ceph-osd2:vg_ceph_20/lv_ceph_20


# ravi prajapati
# inravpra
# igor versluis

# inigover
# Root@!23


# https://10.204.68.46:8443/
# admin
# ravi@123

# sudo ceph osd pool create cephfs_data
# sudo ceph osd pool create cephfs_metadata
# sudo ceph fs new cephfs cephfs_metadata cephfs_data
# sudo ceph fs ls